{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 Urbain Vaes. All rights reserved.\n",
    "#\n",
    "# This work is licensed under the terms of the MIT license.\n",
    "# For a copy, see <https://opensource.org/licenses/MIT>.\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib (for plots)\n",
    "# In older versions of matplotlib, this needs to appear in a different cell as\n",
    "# the import of pyplot: https://github.com/ipython/ipython/issues/11098\n",
    "matplotlib.rc('font', size=20)\n",
    "matplotlib.rc('font', family='serif')\n",
    "matplotlib.rc('text', usetex=False)\n",
    "matplotlib.rc('figure', figsize=(14, 8))\n",
    "matplotlib.rc('lines', linewidth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the Linear Congruential Generator with the default parameters given in the lecture notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcg(n, x0, M=2**32, a=22695477, c=1):\n",
    "    \"\"\" Generate pseudo-random numbers with the LCG method\n",
    "\n",
    "    The LCG is based on the iteration\n",
    "\n",
    "        x(n+1) = (a*x(n) + c) % M\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : integer\n",
    "        The number of pseudo-random variables to generate\n",
    "    x0 : integer\n",
    "        The seed\n",
    "    M : integer\n",
    "        The modulus\n",
    "    a : integer\n",
    "        The multiplier\n",
    "    c : integer\n",
    "        The increment\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A Numpy array of `n` pseudo-random varibales\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The default parameters are the ones used by glibc\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    result = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x0 = (a*x0 + c) % M\n",
    "        result[i] = x0/float(M)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate $10^5$ random variables using our random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With glibc parameters (good generator)\n",
    "x = lcg(10**5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is no universal and foolproof test that can guarantee that a RNG or PRNG is good, in practice a number of tests have been developed to detect whether a simulation method is bad: the Kolmogorov-Smirnov test (see below), the $\\chi^2$ test, etc. If these tests fail, we can reject the hypothesis that the numbers produced were drawn from a uniform distribution.\n",
    "\n",
    "Before presenting the Kolmogorov-Smirnov test, let us check that the empirical PDF of our data is close to the expected one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# Number of points for the plots\n",
    "n_grid = 200\n",
    "\n",
    "# Plot histogram (an approximation of the PDF) and the expected PDF\n",
    "u = np.linspace(0, 1, n_grid)\n",
    "exact_pdf = np.ones(n_grid)\n",
    "ax[0].hist(x, bins=20, density=True)\n",
    "ax[0].plot(u, exact_pdf)\n",
    "ax[0].set_title(\"Histogram and exact PDF\")\n",
    "\n",
    "\n",
    "# Pair the values in the array x 2 by 2, calculate the difference between\n",
    "# the elements of each pair, and plot the results in a histogram.\n",
    "#\n",
    "# Note: the difference of two uniformly-distributed random\n",
    "# variables has PDF (1 - |x|) on [-1, 1].\n",
    "x_odd = x[0::2]\n",
    "x_even = x[1::2]\n",
    "u = np.linspace(-1, 1, n_grid)\n",
    "exact_pdf = (1 - abs(u))\n",
    "ax[1].hist(x_odd - x_even,bins=20, density=True)\n",
    "ax[1].plot(u, exact_pdf)\n",
    "ax[1].set_title(\"Histogram and exact PDF of the difference\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kolmogorov-Smirnov test\n",
    "\n",
    "Consider the empirical CDF $F_N$ associated to $N$ random uniformly-distributed samples:\n",
    "$$\n",
    "F_N(x) = \\frac{1}{N} \\sum_{i=1}^N I_{(-\\infty, x]}(X_i)\n",
    "$$\n",
    "where $I_{(-\\infty, x]}$ is the indicator function of the interval $(-\\infty, x]$.\n",
    "By the law of large numbers, for all $x$ it holds that\n",
    "$$\n",
    "F_N(x) = \\frac{1}{N} \\sum_{i=1}^N I_{(-\\infty, x]}(X_i) \\xrightarrow{\\text{a.s. as }N \\to \\infty} \\mathbb E(I_{(-\\infty, x]}(X_i)) = \\mathbb P(X_i \\leq x) = F(x) := \\max(0, \\min(1, x)),\n",
    "$$\n",
    "where $F$ is the CDF of the uniform distribution.\n",
    "\n",
    "In fact, we can show more\n",
    "\n",
    "- *Glivenko-Cantelli theorem*: $D_N := \\sup_{x \\in \\mathbb R} |F_N(x) - F(x)| \\to 0$ almost surely as $N \\to \\infty$.\n",
    "\n",
    "- *Kolmogorov theorem*: $\\sqrt{n} D_N \\to K$ in distribution, where $K$ is distributed according to the Kolmogorov distribution.  The CDF of $K$ is given by:\n",
    "$$\n",
    "\\mathbb{P}(K\\leq x)=1-2\\sum_{k=1}^\\infty (-1)^{k-1} e^{-2k^2 x^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort random samples and calculate the CDF\n",
    "x = np.sort(x)\n",
    "cdf = np.arange(1, len(x) + 1)/len(x)\n",
    "cdf_shifted = np.arange(0, len(x))/len(x)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "# ax[0].hist(x, cumulative=True, bins=100, density=True, histtype='step')\n",
    "ax[0].plot(x, cdf)\n",
    "ax[0].plot(x, x)\n",
    "ax[0].set_title(\"Empirical and exact CDFs\")\n",
    "ax[1].plot(x, cdf - x)\n",
    "ax[1].plot(x, 0*x)\n",
    "ax[1].set_title(\"Difference between empirical and exact CDFs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Now let us calculate the sup norm of the difference between the empirical CDF,\n",
    "# based on the data in `x`, and the exact CDF of the uniform distribution.\n",
    "error_sup = max(np.max(abs(cdf - x)), np.max(abs(cdf_shifted - x)))\n",
    "normalized_statistic = np.sqrt(len(x)) * error_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate approximately the probability (`pvalue` below) of observing `error_sup` greater than or equal to  what we observed when assuming that the\n",
    "elements of `x` are drawn from a uniform distribution. This is an approximation because, for finite $N$, our test statistic is not exactly distributed according to the Kolmogorov distribution.\n",
    "\n",
    "Below we also check that our results are consistent with those obtained by an application of the function `kstest` in `scipy.stats`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvalue_kolmogorov(y, truncation=10**5):\n",
    "    \"\"\" Calculate the probability that K â‰¥ y, if K follows the Kolmogorov\n",
    "    distribution\n",
    "\n",
    "    y : float\n",
    "    truncation: integer\n",
    "        index at which the series is truncated\n",
    "    \"\"\"\n",
    "    return 2*np.sum([(-1)**(k-1)*np.exp(-2*k**2*y**2)\n",
    "                     for k in range(1, truncation)])\n",
    "\n",
    "\n",
    "# We know that, asymptotically, `error_sup` follows the Kolmogorov\n",
    "# distribution.\n",
    "pvalue = pvalue_kolmogorov(normalized_statistic)\n",
    "print(\"Pvalue calculated: {}\".format(pvalue))\n",
    "\n",
    "# Check that we obtain the correct results\n",
    "statistic, pvalue = stats.kstest(x, 'uniform', mode='asymp')\n",
    "print(\"Pvalue calculated with SciPy: {}\".format(pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we can't reject the hypothesis that our samples were drawn from a true uniform distribution. The Mersenne-Twister algorithm gives similar results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(10**5)\n",
    "statistic, pvalue = stats.kstest(x, 'uniform', mode='asymp')\n",
    "print(\"Pvalue calculated with SciPy: {}\".format(pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: `pvalue` is the probablility of observing `error_sup`\n",
    "greater than or equal to that what we observed when assuming that the\n",
    "elements of `x` are drawn from a uniform distribution."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
